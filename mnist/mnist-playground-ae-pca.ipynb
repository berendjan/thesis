{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport glob\nimport os\nimport nibabel as nib               # NIfTI format -> .nii files\nimport matplotlib.pyplot as plt\nimport torch\nfrom torch import nn, optim, Tensor\nfrom torch.nn import functional as F\nfrom torch import optim\nfrom torch.utils.data import Dataset, DataLoader\nimport torchvision\nfrom torchvision import transforms, utils\nimport numbers\nimport time\n\n\nfrom sklearn.decomposition import PCA","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"torch.cuda.manual_seed(5)\ntorch.manual_seed(5)\nnp.random.seed(42)\n# random.seed(42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"transform = torchvision.transforms.Compose([\n    torchvision.transforms.ToTensor(),\n    #torchvision.transforms.Normalize(mean=(0.1307,), std=(0.3081,))\n])\n\ntrain_dataset = torchvision.datasets.MNIST(\n    root=\"~/torch_datasets\", train=True, transform=transform, download=True\n)\n\ntest_dataset = torchvision.datasets.MNIST(\n    root=\"~/torch_datasets\", train=False, transform=transform, download=True\n)\n\ntrain_loader = torch.utils.data.DataLoader(\n    train_dataset, batch_size=128, shuffle=True, num_workers=4, pin_memory=True\n)\n\ntest_loader = torch.utils.data.DataLoader(\n    test_dataset, batch_size=32, shuffle=False, num_workers=4\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class LinearAutoEncoder(nn.Module):\n    def __init__(self):\n        super(LinearAutoEncoder, self, ).__init__()\n        self.layer1 = nn.Linear(in_features=28*28, out_features=128)\n#         self.layer2 = nn.Linear(in_features=128, out_features=128)\n#         self.layer3 = nn.Linear(in_features=128, out_features=128)\n#         self.layer4 = torch.nn.utils.weight_norm(nn.Linear(in_features=128, out_features=28*28))\n        self.layer4 = nn.Linear(in_features=128, out_features=28*28)\n        \n    def forward(self, tensor):\n        tensor = tensor.view(-1, 784)\n        tensor = self.layer1(tensor)\n#         tensor = torch.relu(tensor)\n#         tensor = self.layer2(tensor)\n#         tensor = torch.relu(tensor)\n#         tensor = self.layer3(tensor)\n#         tensor = torch.relu(tensor)\n        tensor = self.layer4(tensor)\n#         tensor = torch.relu(tensor)\n        tensor = tensor.view(-1, 1, 28, 28)\n        return tensor","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#  use gpu if available\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nstorage = {}\n# storage{model_label}[run][epoch]{time_and_loss_label}[data]\n\nepochs = 40\nruns = 1\nlr_start = 1e-3\n\nlin_label = \"linear\"\npca_label = \"pca\"\n\nloss_label = \"loss\"\ntime_label = \"time\"\n\ndef train(mdl, label, lr_start, epochs, runs):\n    \n    # add result dict for each epoch for each run\n    storage[label] = [[{} for i in range(epochs)] for j in range(runs)]\n    \n    # add storage for models\n    models = []\n    \n    for run in range(runs):\n        \n        # initialize model class and send to device\n        model = mdl().to(device)\n        \n        # initialize new optimizer with model parameters\n        optimizer = optim.Adam(model.parameters(), lr=lr_start)\n        \n        # initialize criterion\n        criterion = nn.MSELoss()\n            \n        # start timing for epochs\n        start_time = time.time()\n\n#         scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.1)\n\n        for epoch in range(epochs):\n\n            # initialize the loss\n            loss = 0\n\n            for batch_features, _ in train_loader:\n\n                # reshape mini-batch data to [N, 784] matrix\n                # load it to the active device\n                batch_features = batch_features.to(device)\n\n                # reset the gradients back to zero\n                # PyTorch accumulates gradients on subsequent backward passes\n                optimizer.zero_grad()\n\n                # compute reconstructions\n                outputs = model(batch_features)\n\n                # compute training reconstruction loss\n                train_loss = criterion(outputs, batch_features)\n\n                # compute accumulated gradients\n                train_loss.backward()\n\n                # perform parameter update based on current gradients\n                optimizer.step()\n\n                # add the mini-batch training loss to epoch loss\n                loss += train_loss.item()\n\n            # learning rate scheduler\n#             scheduler.step()\n\n            # compute the epoch training loss\n            loss = loss / len(train_loader)\n\n            # epoch duration\n            dur = time.time() - start_time\n\n            # display the epoch training loss\n            print(\"model : {}, run : {}/{}, epoch : {}/{}, loss = {:.6f}, duration = {:.1f}, lr : {}\".format(label, run + 1, runs, epoch + 1, epochs, loss, dur, get_lr(optimizer)))\n\n            storage[label][run][epoch][loss_label] = loss\n            storage[label][run][epoch][time_label] = dur\n            \n        # Add to models\n        models.append(model)\n    \n    # return list of models and storage\n    return models\n        \ndef get_lr(optimizer):\n    for param_group in optimizer.param_groups:\n        return param_group['lr']\n        \nlin_models = train(LinearAutoEncoder, lin_label, lr_start, epochs, runs)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# GET FULL DATA SET IN B WITH SHAPE (60000, 28, 28)\nprint(train_dataset[59999][0].shape)\ntest = [train_dataset[i][0] for i in range(len(train_dataset))]\nb = torch.Tensor(len(train_dataset), 1, 28, 28)\ntorch.cat(test, out=b)\nprint(b.shape)\n\n\n\n# # GET FIRST BATCH OF MACHINE\nprint('\\nlinear')\n# lin_models[0].float()?\nmean_pixels = torch.mean(b, axis=0)\n# centered = b - mean_pixels\nfirst_batch = b.unsqueeze(1)[:128,:,:,:]\nfirst_batch = first_batch[:,:,:,:].to(device)\n\n# # RESULT OF LINEAR AE\nlin_models[0].float()\nlin_result = lin_models[0](first_batch)\nlin_models[0].double()\n\n\n\n# FIT PCA ON DATASET\nprint('\\nsckit pca')\npca = PCA(n_components=128)\npca.fit(b.view(-1, 784))\neigen_values = pca.explained_variance_\neigen_vectors = pca.components_\nprojected = pca.transform(b.view(-1, 784))\nreconstructed = torch.from_numpy(np.matmul(projected, eigen_vectors)).view(-1, 28, 28)\n\n# MANUAL PCA\nprint('\\nmanual pca')\nmean = torch.mean(b.view(-1, 784), axis=0)\ncent = b.view(-1, 784) - mean\ncov = np.cov(cent.T)\neigvals, eigvecs = np.linalg.eigh(cov)\nprincipal_eigvals = eigvals[np.argsort(-eigvals)[:128]]\nprincipal_eigvecs = eigvecs[:, np.argsort(-eigvals)[:128]].T\nproj = principal_eigvecs.dot(cent.T).T\nmanual_reconstructed = torch.from_numpy(np.matmul(proj, principal_eigvecs)).view(-1, 28, 28)\n\n\n\n\nfig, axes = plt.subplots(4, 5, figsize=(15,10))\nfor i in range(5):\n    axes[0][i].imshow(first_batch[i].cpu().squeeze(0) - mean_pixels, cmap=\"gray\")\n    axes[1][i].imshow(lin_result[i].cpu().detach().squeeze(0) - mean_pixels, cmap=\"gray\")\n    axes[2][i].imshow(reconstructed[i], cmap=\"gray\")\n    axes[3][i].imshow(manual_reconstructed[i], cmap=\"gray\")\n\naxes[0][0].set_ylabel(\"original\")\naxes[1][0].set_ylabel(\"linear\")\naxes[2][0].set_ylabel(\"sckit-pca\")\naxes[3][0].set_ylabel(\"manual-pca\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# GET FULL DATA SET IN B WITH SHAPE (60000, 28, 28)\nprint(train_dataset[59999][0].shape)\ntest = [train_dataset[i][0] for i in range(len(train_dataset))]\nb = torch.Tensor(len(train_dataset), 1, 28, 28)\ntorch.cat(test, out=b)\nprint(b.shape)\n\n\n\n# # GET FIRST BATCH OF MACHINE\nprint('\\nlinear')\n# lin_models[0].float()?\nmean_pixels = torch.mean(b, axis=0)\n# centered = b - mean_pixels\nfirst_batch = b.unsqueeze(1)[:128,:,:,:]\nfirst_batch = first_batch[:,:,:,:].to(device)\n\n# # RESULT OF LINEAR AE\nlin_models[0].float()\nlin_result = lin_models[0](first_batch)\nlin_models[0].double()\n\n\n\n# FIT PCA ON DATASET\nprint('\\nsckit pca')\npca = PCA(n_components=128)\npca.fit(b.view(-1, 784))\neigen_values = pca.explained_variance_\neigen_vectors = pca.components_\nprojected = pca.transform(b.view(-1, 784))\nreconstructed = torch.from_numpy(np.matmul(projected, eigen_vectors)).view(-1, 28, 28)\n\n# MANUAL PCA\nprint('\\nmanual pca')\nmean = torch.mean(b.view(-1, 784), axis=0)\ncent = b.view(-1, 784) - mean\ncov = np.cov(cent.T)\neigvals, eigvecs = np.linalg.eigh(cov)\nprincipal_eigvals = eigvals[np.argsort(-eigvals)[:128]]\nprincipal_eigvecs = eigvecs[:, np.argsort(-eigvals)[:128]].T\nproj = principal_eigvecs.dot(cent.T).T\nmanual_reconstructed = torch.from_numpy(np.matmul(proj, principal_eigvecs)).view(-1, 28, 28)\n\n\n\n\nfig, axes = plt.subplots(4, 5, figsize=(15,10))\nfor i in range(5):\n    axes[0][i].imshow(first_batch[i].cpu().squeeze(0), cmap=\"gray\")\n    axes[1][i].imshow(lin_result[i].cpu().detach().squeeze(0), cmap=\"gray\")\n    axes[2][i].imshow(reconstructed[i] + mean_pixels, cmap=\"gray\")\n    axes[3][i].imshow(manual_reconstructed[i] + mean_pixels, cmap=\"gray\")\n\naxes[0][0].set_ylabel(\"original\")\naxes[1][0].set_ylabel(\"linear\")\naxes[2][0].set_ylabel(\"sckit-pca\")\naxes[3][0].set_ylabel(\"manual-pca\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from numpy import array\nfrom numpy import mean\nfrom numpy import cov\nfrom numpy.linalg import eigh\n# define a matrix\nA = array([[1, 3, 2], [6, 4, 1], [5, 6, 5]])\nprint(A)\n# calculate the mean of each column\nM = mean(A.T, axis=1)\nprint(M)\n# center columns by subtracting column means\nC = A - M\nprint(C)\n# calculate covariance matrix of centered matrix\nV = cov(C.T)\nprint(V)\n# eigendecomposition of covariance matrix\nvalues, vectors = eigh(V)\nprincipal_values = values[np.argsort(-values)[:2]]\nprincipal_vectors = vectors[:,np.argsort(-values)[:2]].T\nprint(principal_vectors)\nprint(principal_values)\n# project data\nP = principal_vectors.dot(C.T).T\nprint(P)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# Principal Component Analysis\nfrom numpy import array\nfrom sklearn.decomposition import PCA\n# define a matrix\nA = array([[1, 3, 2], [6, 4, 1], [5, 6, 5]])\nprint(A)\n# create the PCA instance\npca = PCA(2)\n# fit on data\npca.fit(A)\n# access values and vectors\nprint(pca.components_)\nprint(pca.explained_variance_)\n# transform data\nB = pca.transform(A)\nprint(B)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}